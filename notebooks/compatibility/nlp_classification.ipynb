{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "still-resident",
   "metadata": {},
   "source": [
    "# HuggingFace: Active Learning for NLP Classification\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baal-org/baal/blob/master/notebooks/compatibility/nlp_classification.ipynb)\n",
    "\n",
    "Learn how to use Baal with [HuggingFace](https://huggingface.co/transformers/main_classes/trainer.html).\n",
    " Any model which could be trained by HuggingFace trainer and has `Dropout` layers could be used in the same manner.\n",
    "\n",
    "We will use the `Yelp Review` dataset and `BertForSequenceClassification` as the model for the purpose of this tutorial. As usual, we need to first download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "id": "sixth-wound",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-06-03T23:33:54.352758Z"
    }
   },
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "pretrained_weights = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=pretrained_weights)\n",
    "\n",
    "datasets = load_dataset(\"yelp_review_full\", cache_dir=\"/tmp\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "datasets = datasets.map(tokenize_function, batched=True)\n",
    "\n",
    "ds = datasets['train']\n",
    "eval_ds = datasets['test']"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-tennessee",
   "metadata": {},
   "source": [
    "## ActiveLearning Dataset\n",
    "In order to create an active learning dataset, we need to wrap the dataset with `baal.ActiveLearningDataset`.\n",
    "This requires a `torch.utils.Dataset` so we propose a `baal.active.HuggingFaceDataset` that can take a HuggingFace dataset\n",
    "and perform the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "id": "liquid-replacement",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from baal import ActiveLearningDataset\n",
    "\n",
    "active_set = ActiveLearningDataset(dataset=ds)\n",
    "active_set.can_label = False  # Need to manually do this for research\n",
    "\n",
    "# lets randomly label 100 samples, therefore len(active_set) should be 100\n",
    "active_set.label_randomly(100)\n",
    "assert len(active_set) == 100\n",
    "print(len(active_set.pool))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ready-participation",
   "metadata": {},
   "source": [
    "## Active Learning Model\n",
    "The process of making a model bayesian is exactly the same as before. In this case, we will get the `Bert` model and use `baal.bayesian.dropout.patch_module` to make the dropout layer stochastic at inference time. "
   ]
  },
  {
   "cell_type": "code",
   "id": "baking-coalition",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from baal.bayesian.dropout import patch_module\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model_name_or_path=pretrained_weights,\n",
    "                                                           num_labels=5)\n",
    "model = patch_module(model)\n",
    "if use_cuda:\n",
    "    model.cuda()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eleven-portugal",
   "metadata": {},
   "source": [
    "## Heuristic\n",
    "\n",
    "As already implemented and useful in all classification cases, we continue using `BALD` as our active learning heuristic.\n",
    "\n",
    "Note: ActiveLearning for NLP tasks is an open and challenging field and hence, desiging a proper heuristic is out of the scope of this tutorial.\n",
    "We encourage any pull request that would propose better heuristics."
   ]
  },
  {
   "cell_type": "code",
   "id": "cooperative-constant",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "from baal.active import get_heuristic\n",
    "\n",
    "heuristic = get_heuristic('bald')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "listed-kelly",
   "metadata": {},
   "source": [
    "## HugginFace Trainer Wrapper\n",
    "\n",
    "If you are not familiar with the HuggingFace trainer module please start [here](https://huggingface.co/transformers/main_classes/trainer.html).\n",
    "HuggingFace Trainer is one of the most popular library to train Transformer models.\n",
    "In order to do active learning, we need the prediction to be run over every sample in pool for number of iterations and hence our wrapper `baal.BaalTransformersTrainer` will provide this functionality on top of the provided functionalities in the `Trainer` module.\n",
    "In the rest of this tutorial, we show how to initialize the `baal.active.active_loop.ActiveLearningLoop` and how to do Active Training."
   ]
  },
  {
   "cell_type": "code",
   "id": "moving-olive",
   "metadata": {},
   "source": [
    "from baal.active.stopping_criteria import LabellingBudgetStoppingCriterion\n",
    "from baal.active.heuristics import BALD\n",
    "from transformers import TrainingArguments\n",
    "from baal.transformers_trainer_wrapper import BaalTransformersTrainer\n",
    "from baal.experiments.base import ActiveLearningExperiment\n",
    "\n",
    "#Initialization for the huggingface trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='.',  # output directory\n",
    "    num_train_epochs=3,  # total # of training epochs per AL step\n",
    "    per_device_train_batch_size=4,  # batch size per device during training\n",
    "    per_device_eval_batch_size=4,  # batch size for evaluation\n",
    "    weight_decay=0.01,  # strength of weight decay\n",
    "    logging_dir='.',  # directory for storing logs\n",
    ")\n",
    "\n",
    "# create the trainer through Baal Wrapper\n",
    "baal_trainer = BaalTransformersTrainer(model=model,\n",
    "                                       args=training_args,\n",
    "                                       train_dataset=active_set,\n",
    "                                       tokenizer=tokenizer)\n",
    "\n",
    "experiment = ActiveLearningExperiment(trainer=baal_trainer,\n",
    "                                      al_dataset=active_set,\n",
    "                                      eval_dataset=eval_ds, heuristic=heuristic, query_size=50, iterations=20,\n",
    "                                      criterion=LabellingBudgetStoppingCriterion(active_dataset=active_set,\n",
    "                                                                                 labelling_budget=100))\n",
    "experiment.start()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "10e8ec667616de39",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
