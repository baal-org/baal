{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use BaaL with Pytorch Lightning\n",
    "\n",
    "In this notebook we'll go through an example of how to build a project with Baal and Pytorch Lightning\n",
    "\n",
    "**Useful resources:**\n",
    "\n",
    "* [Pytorch Lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "* [Collection of notebooks with other relevant examples](https://github.com/ElementAI/baal/tree/master/notebooks)\n",
    "\n",
    "**NOTE** The API of `ActiveLearningMixin` and `BaalTrainer` are subject to change as we are looking for feedback\n",
    "from the community. If you want to help us making this API better, please come to our Gitter or submit an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "from baal.active import ActiveLearningDataset\n",
    "from baal.active.heuristics import BALD\n",
    "from baal.bayesian.dropout import patch_module\n",
    "from baal.utils.pytorch_lightning import ActiveLearningMixin, BaalTrainer, BaaLDataModule, ResetCallback\n",
    "from pytorch_lightning import LightningModule\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model definition\n",
    "\n",
    "Bellow you can see an example using VGG16\n",
    "\n",
    "Note the `ActiveLearningMixin` which we will use to perform active learning.\n",
    "This Mixin expects an active dataset and the following keys in the `hparams`:\n",
    "\n",
    "```python\n",
    "iterations: int # How many MC sampling to perform at prediction time.\n",
    "replicate_in_memory: bool # Whether to perform MC sampling by replicating the batch `iterations` times.\n",
    "```\n",
    "\n",
    "If you want to modify how the MC sampling is made, you can overwrite `predict_step`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(LightningModule, ActiveLearningMixin):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.name = \"VGG16\"\n",
    "        self.version = \"0.0.1\"\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # We use `patch_module` to swap Dropout modules in the model\n",
    "        # for our implementation which enables MC-Dropou\n",
    "        self.vgg16 = patch_module(vgg16(num_classes=self.hparams.num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        :param batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log(\"train_loss\", loss_val, prog_bar=True, on_epoch=True)\n",
    "        return loss_val\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y_hat, y)\n",
    "\n",
    "        self.log(\"test_loss\", loss_val, prog_bar=True, on_epoch=True)\n",
    "        return loss_val\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        return whatever optimizers we want here\n",
    "        :return: list of optimizers\n",
    "        \"\"\"\n",
    "        optimizer = optim.SGD(self.parameters(), lr=self.hparams.learning_rate, momentum=0.9, weight_decay=5e-4)\n",
    "        return [optimizer], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class HParams:\n",
    "    batch_size: int = 10\n",
    "    data_root: str = '/tmp'\n",
    "    num_classes: int = 10\n",
    "    learning_rate: float = 0.001\n",
    "    query_size: int = 100\n",
    "    iterations: int = 20\n",
    "    replicate_in_memory: bool = True\n",
    "    gpus: int = 1\n",
    "\n",
    "hparams = HParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule\n",
    "\n",
    "We support `pl.DataModule`, here is how you can define it. By using `BaaLDataModule`, you do not\n",
    "have to implement `pool_dataloader` which is the DataLoader that runs on the pool of unlabelled examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class Cifar10DataModule(BaaLDataModule):\n",
    "    def __init__(self, data_root, batch_size):\n",
    "        train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                              transforms.ToTensor()])\n",
    "        test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        active_set = ActiveLearningDataset(\n",
    "            CIFAR10(data_root, train=True, transform=train_transform, download=True),\n",
    "            pool_specifics={\n",
    "                'transform': test_transform\n",
    "            })\n",
    "        self.test_set = CIFAR10(data_root, train=False, transform=test_transform, download=True)\n",
    "        super().__init__(active_dataset=active_set, batch_size=batch_size,\n",
    "                         train_transforms=train_transform,\n",
    "                         test_transforms=test_transform)\n",
    "\n",
    "    def train_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        return DataLoader(self.active_dataset, self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs) -> DataLoader:\n",
    "        return DataLoader(self.test_set, self.batch_size, shuffle=False, num_workers=4)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "We now have all the pieces to start our experiment.\n",
    "\n",
    "### Initial labelling\n",
    "\n",
    "To kickstart active learning, we will randomly select items to be labelled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = Cifar10DataModule(hparams.data_root, hparams.batch_size)\n",
    "data_module.active_dataset.label_randomly(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating `BALD`\n",
    "\n",
    "This is used to rank the uncertainty. More info [here](https://baal.readthedocs.io/en/latest/notebooks/baal_prod_cls.html#Heuristics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic = BALD()\n",
    "model = VGG16(**asdict(hparams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trainer to generate predictions\n",
    "\n",
    "Note that we use the BaalTrainer which inherits the usual Pytorch Lightning Trainer.\n",
    "The BaaLTrainer will take care of the active learning part by performing `predict_on_dataset` on the pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BaalTrainer(dataset=data_module.active_dataset,\n",
    "                      heuristic=heuristic,\n",
    "                      query_size=hparams.query_size,\n",
    "                      max_epochs=10, default_root_dir=hparams.data_root,\n",
    "                      gpus=hparams.gpus,\n",
    "                      callbacks=[ResetCallback(copy.deepcopy(model.state_dict()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model and perform Active learning\n",
    "\n",
    "Our experiment steps are as follow:\n",
    "\n",
    "1. Train on the labelled dataset.\n",
    "2. Evaluate ourselves on a held-out set.\n",
    "3. Label the top-k most uncertain examples.\n",
    "4. Go back to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_STEPS = 100\n",
    "\n",
    "for al_step in range(AL_STEPS):\n",
    "    print(f'Step {al_step} Dataset size {len(data_module.active_dataset)}')\n",
    "    trainer.fit(model, datamodule=data_module)  # Train the model on the labelled set.\n",
    "    trainer.test(model, datamodule=data_module)  # Get test performance.\n",
    "    should_continue = trainer.step(model, datamodule=data_module)  # Label the top-k most uncertain examples.\n",
    "    if not should_continue:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}