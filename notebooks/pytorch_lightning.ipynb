{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import OrderedDict\n",
    "\n",
    "from collections.abc import Sequence\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import structlog\n",
    "import torch\n",
    "from pydantic import BaseModel\n",
    "from pytorch_lightning import LightningModule, Trainer, Callback\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from baal.active import ActiveLearningDataset, ActiveLearningLoop\n",
    "from baal.active.heuristics import BALD\n",
    "from baal.modelwrapper import mc_inference\n",
    "from baal.utils.cuda_utils import to_cuda\n",
    "from baal.utils.iterutils import map_on_tensor\n",
    "\n",
    "from baal.utils.pytorch_lightning import ActiveLearningMixin, ResetCallback, BaalTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to implement our model based on the PytorchLightning specifications. Bellow you can see an example using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(ActiveLearningMixin, LightningModule):\n",
    "    def __init__(self, active_dataset, hparams):\n",
    "        super().__init__()\n",
    "        self.name = \"VGG16\"\n",
    "        self.version = \"0.0.1\"\n",
    "        self.active_dataset = active_dataset\n",
    "        self.hparams = hparams\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "\n",
    "        self.train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                                   transforms.ToTensor()])\n",
    "        self.test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        self.vgg16 = vgg16(num_classes=self.hparams.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)\n",
    "\n",
    "    def log_hyperparams(self, *args):\n",
    "        print(args)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        :param batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y_hat, y)\n",
    "\n",
    "        tqdm_dict = {'train_loss': loss_val}\n",
    "        output = OrderedDict({\n",
    "            'loss': loss_val,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        })\n",
    "        return output\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y, y_hat)\n",
    "\n",
    "        tqdm_dict = {'val_loss': loss_val}\n",
    "        output = OrderedDict({\n",
    "            'loss': loss_val,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        })\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        return whatever optimizers we want here\n",
    "        :return: list of optimizers\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.active_dataset, self.hparams.batch_size, shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        ds = CIFAR10(root=self.hparams.data_root, train=False,\n",
    "                     transform=self.test_transform, download=True)\n",
    "        return DataLoader(ds, self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=4)\n",
    "\n",
    "    def pool_loader(self):\n",
    "        return DataLoader(self.active_dataset.pool, self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=4)\n",
    "\n",
    "    def log_metrics(self, metrics, step_num):\n",
    "        print('Epoch', step_num, metrics)\n",
    "\n",
    "    def agg_and_log_metrics(self, metrics, step):\n",
    "        self.log_metrics(metrics, step)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.epoch_end(outputs)\n",
    "\n",
    "    def epoch_end(self, outputs):\n",
    "        out = {}\n",
    "        if len(outputs) > 0:\n",
    "            out = {key: torch.stack([x[key] for x in outputs]).mean() for key in outputs[0].keys()}\n",
    "        return out\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.epoch_end(outputs)\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to specify our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(BaseModel):\n",
    "    batch_size: int = 10\n",
    "    data_root: str = '/tmp'\n",
    "    num_classes: int = 10\n",
    "    learning_rate: float = 0.001\n",
    "    query_size: int = 100\n",
    "    max_sample: int = -1\n",
    "    iterations: int = 20\n",
    "    replicate_in_memory: bool = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the transformations to be used with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                    transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've defined above our hyperparameters using a Pydantic based class which will ensure we're using the correct data types. We instntiate that class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = HParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We instantiate an ActiveLearning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "active_set = ActiveLearningDataset(\n",
    "        CIFAR10(hparams.data_root, train=True, transform=train_transform, download=True),\n",
    "        pool_specifics={\n",
    "            'transform': test_transform\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label a few random items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_set.label_randomly(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic = BALD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(active_set, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    }
   ],
   "source": [
    "trainer = BaalTrainer(callbacks=[ResetCallback(copy.deepcopy(model.state_dict()))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = ActiveLearningLoop(active_set, get_probabilities=trainer.predict_on_dataset_generator,\n",
    "                          heuristic=heuristic,\n",
    "                          ndata_to_label=hparams.query_size,\n",
    "                          max_sample=hparams.max_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_STEPS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params\n",
      "-----------------------------------------------\n",
      "0 | criterion | CrossEntropyLoss | 0     \n",
      "1 | vgg16     | VGG              | 134 M \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 DS size 10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3ecfb0b79c4f5b88895bbe4fc558a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for al_step in range(AL_STEPS):\n",
    "    print(f'Step {al_step} DS size {len(active_set)}')\n",
    "    trainer.fit(model)\n",
    "    should_continue = loop.step()\n",
    "    if not should_continue:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
