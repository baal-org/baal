{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook we'll go through an example of how to build a project with Baal and Pytorch Lightning\n",
    "---\n",
    "#### Useful resources:\n",
    "* [Pytorch Lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/)\n",
    "* [Collection of notebooks with other relevant examples](https://github.com/ElementAI/baal/tree/master/notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import copy\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import OrderedDict\n",
    "\n",
    "from collections.abc import Sequence\n",
    "\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import structlog\n",
    "import torch\n",
    "from pydantic import BaseModel\n",
    "from pytorch_lightning import LightningModule, Trainer, Callback\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from baal.active import ActiveLearningDataset, ActiveLearningLoop\n",
    "from baal.active.heuristics import BALD\n",
    "from baal.modelwrapper import mc_inference\n",
    "from baal.utils.cuda_utils import to_cuda\n",
    "from baal.utils.iterutils import map_on_tensor\n",
    "from baal.bayesian.dropout import patch_module\n",
    "\n",
    "from baal.utils.pytorch_lightning import ActiveLearningMixin, ResetCallback, BaalTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to implement our model based on the PytorchLightning specifications. Bellow you can see an example using VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(ActiveLearningMixin, LightningModule):\n",
    "    def __init__(self, active_dataset, hparams):\n",
    "        super().__init__()\n",
    "        self.name = \"VGG16\"\n",
    "        self.version = \"0.0.1\"\n",
    "        self.active_dataset = active_dataset\n",
    "        self.hparams = hparams\n",
    "        self.criterion = CrossEntropyLoss()\n",
    "\n",
    "        self.train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                                   transforms.ToTensor()])\n",
    "        self.test_transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        # `patch_module` modifies the torch.nn.Module to use Bayesian dropout\n",
    "        self.vgg16 = patch_module(vgg16(num_classes=self.hparams.num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.vgg16(x)\n",
    "\n",
    "    def log_hyperparams(self, *args):\n",
    "        print(args)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Lightning calls this inside the training loop\n",
    "        :param batch:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # forward pass\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y_hat, y)\n",
    "\n",
    "        tqdm_dict = {'train_loss': loss_val}\n",
    "        output = OrderedDict({\n",
    "            'loss': loss_val,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        })\n",
    "        return output\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "\n",
    "        # calculate loss\n",
    "        loss_val = self.criterion(y, y_hat)\n",
    "\n",
    "        tqdm_dict = {'val_loss': loss_val}\n",
    "        output = OrderedDict({\n",
    "            'loss': loss_val,\n",
    "            'progress_bar': tqdm_dict,\n",
    "            'log': tqdm_dict\n",
    "        })\n",
    "        return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        return whatever optimizers we want here\n",
    "        :return: list of optimizers\n",
    "        \"\"\"\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.hparams.learning_rate)\n",
    "        return [optimizer], []\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.active_dataset, self.hparams.batch_size, shuffle=True,\n",
    "                          num_workers=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        ds = CIFAR10(root=self.hparams.data_root, train=False,\n",
    "                     transform=self.test_transform, download=True)\n",
    "        return DataLoader(ds, self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=4)\n",
    "\n",
    "    def pool_loader(self):\n",
    "        return DataLoader(self.active_dataset.pool, self.hparams.batch_size, shuffle=False,\n",
    "                          num_workers=4)\n",
    "\n",
    "    def log_metrics(self, metrics, step_num):\n",
    "        print('Epoch', step_num, metrics)\n",
    "\n",
    "    def agg_and_log_metrics(self, metrics, step):\n",
    "        self.log_metrics(metrics, step)\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        return self.epoch_end(outputs)\n",
    "\n",
    "    def epoch_end(self, outputs):\n",
    "        out = {}\n",
    "        if len(outputs) > 0:\n",
    "            out = {key: torch.stack([x[key] for x in outputs]).mean() for key in outputs[0].keys()}\n",
    "        return out\n",
    "\n",
    "    def test_epoch_end(self, outputs):\n",
    "        return self.epoch_end(outputs)\n",
    "\n",
    "    def training_epoch_end(self, outputs):\n",
    "        return self.epoch_end(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to specify our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HParams(BaseModel):\n",
    "    batch_size: int = 10\n",
    "    data_root: str = '/tmp'\n",
    "    num_classes: int = 10\n",
    "    learning_rate: float = 0.001\n",
    "    query_size: int = 100\n",
    "    max_sample: int = -1\n",
    "    iterations: int = 20\n",
    "    replicate_in_memory: bool = True\n",
    "    n_gpus: int = torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the transformations to be used with our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've defined above our hyperparameters using a [Pydantic](https://pydantic-docs.helpmanual.io/) based class which will ensure we're using the correct data types. We instntiate that class here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = HParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We instantiate an ActiveLearning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_set = ActiveLearningDataset(\n",
    "        CIFAR10(hparams.data_root, train=True, transform=train_transform, download=True),\n",
    "        pool_specifics={\n",
    "            'transform': test_transform\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label a few random items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_set.label_randomly(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate `BALD` so we can use its heuristics.\n",
    "##### This is used to rank the uncertainty. More info [here](https://baal.readthedocs.io/en/latest/notebooks/baal_prod_cls.html#Heuristics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heuristic = BALD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16(active_set, hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a trainer to generate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BaalTrainer(max_epochs=10, default_root_dir=hparams.data_root,\n",
    "                      gpus=hparams.n_gpus, distributed_backend='dp' if hparams.n_gpus > 1 else None,\n",
    "                      callbacks=[ResetCallback(copy.deepcopy(model.state_dict()))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap the ActiveLearningDataset with ActiveLearningLoop. This object provides a step method that will be used to label our data during the training loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = ActiveLearningLoop(active_set, get_probabilities=trainer.predict_on_dataset_generator,\n",
    "                          heuristic=heuristic,\n",
    "                          ndata_to_label=hparams.query_size,\n",
    "                          max_sample=hparams.max_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_STEPS = 100\n",
    "\n",
    "for al_step in range(AL_STEPS):\n",
    "    print(f'Step {al_step} DS size {len(active_set)}')\n",
    "    trainer.fit(model)\n",
    "    # Predict on the pool, estimate the uncertainty and label the most uncertain.\n",
    "    should_continue = loop.step()\n",
    "    # If there is work to do, continue\n",
    "    if not should_continue:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
