{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# How to use Deep ensembles in Baal\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baal-org/baal/blob/master/notebooks/deep_ensemble.ipynb)\n",
    "\n",
    "Ensemble are one of the easiest form of Bayesian deep learning.\n",
    " The main drawback from this approach is the important amount of computational resources needed to perform it.\n",
    "  In this notebook, we will present Baal's Ensemble API namely `EnsembleModelWrapper`.\n",
    "\n",
    "\n",
    "This notebook is similar to our notebook on how to do research, we suggest you look at it first if you haven't.\n",
    "\n",
    "#### Additional resources\n",
    "\n",
    "* More info on the inner working of Active Learning Dataset [here](./fundamentals/active-learning.ipynb).\n",
    "* To know more about Bayesian deep learning please see our Literature review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import random\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch.backends\n",
    "from torch import optim, nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "from torchvision.transforms import transforms\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from baal.active import get_heuristic, ActiveLearningDataset\n",
    "from baal.active.active_loop import ActiveLearningLoop\n",
    "from baal.ensemble import EnsembleModelWrapper\n",
    "\n",
    "def vgg16(num_classes):\n",
    "    model = models.vgg16(pretrained=False, num_classes=num_classes)\n",
    "    weights = load_state_dict_from_url('https://download.pytorch.org/models/vgg16-397923af.pth')\n",
    "    weights = {k: v for k, v in weights.items() if 'classifier.6' not in k}\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    return model\n",
    "\n",
    "def weights_reset(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        nn.init.constant_(m.bias, 0)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    epoch: int = 15000//256\n",
    "    batch_size: int = 32\n",
    "    initial_pool: int = 512\n",
    "    query_size: int = 100\n",
    "    lr: float = 0.001\n",
    "    heuristic: str = 'bald'\n",
    "    iterations: int = 5 # Set a low number here since each iteration will train a new model.\n",
    "    training_duration: int = 10\n",
    "        \n",
    "\n",
    "def get_datasets(initial_pool):\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.Resize((32, 32)),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(30),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize(3 * [0.5], 3 * [0.5]), ])\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(3 * [0.5], 3 * [0.5]),\n",
    "        ]\n",
    "    )\n",
    "    train_ds = datasets.CIFAR10('.', train=True,\n",
    "                                transform=transform, target_transform=None, download=True)\n",
    "    \n",
    "    # In a real application, you will want a validation set here.\n",
    "    test_set = datasets.CIFAR10('.', train=False,\n",
    "                                transform=test_transform, target_transform=None, download=True)\n",
    "    \n",
    "    # Here we set `pool_specifics`, where we set the transform attribute for the pool.\n",
    "    active_set = ActiveLearningDataset(train_ds, pool_specifics={'transform': test_transform})\n",
    "\n",
    "    # We start labeling randomly.\n",
    "    active_set.label_randomly(initial_pool)\n",
    "    return active_set, test_set"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "hyperparams = ExperimentConfig()\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.backends.cudnn.benchmark = True\n",
    "random.seed(1337)\n",
    "torch.manual_seed(1337)\n",
    "if not use_cuda:\n",
    "    print(\"warning, the experiments would take ages to run on cpu\")\n",
    "\n",
    "# Get datasets\n",
    "active_set, test_set = get_datasets(hyperparams.initial_pool)\n",
    "\n",
    "heuristic = get_heuristic(hyperparams.heuristic)\n",
    "criterion = CrossEntropyLoss()\n",
    "model = vgg16(num_classes=10)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "optimizer = optim.SGD(model.parameters(), lr=hyperparams.lr, momentum=0.9)\n",
    "\n",
    "# Wraps the model into a usable API.\n",
    "model = EnsembleModelWrapper(model, criterion)\n",
    "\n",
    "# for prediction we use a smaller batchsize\n",
    "# since it is slower\n",
    "active_loop = ActiveLearningLoop(active_set,\n",
    "                                 model.predict_on_dataset,\n",
    "                                 heuristic,\n",
    "                                 hyperparams.query_size,\n",
    "                                 batch_size=1,\n",
    "                                 iterations=hyperparams.iterations,\n",
    "                                 use_cuda=use_cuda,\n",
    "                                 verbose=False)\n",
    "\n",
    "# We will reset the weights at each active learning step.\n",
    "init_weights = deepcopy(model.state_dict())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Presenting EnsembleModelWrapper\n",
    "\n",
    "EnsembleModelWrapper is similar to ModelWrapper, but instead of training a single model, we will train multiple.\n",
    "Each model will start its training from a different set parameters.\n",
    "\n",
    "EnsembleModelWrappe methods:\n",
    "\n",
    "```python\n",
    "class EnsembleModelWrapper:\n",
    "    def add_checkpoint(self):\n",
    "        \"\"\"\n",
    "        Add a checkpoint to the list of weights used for inference.\n",
    "        \"\"\"\n",
    "\n",
    "    def clear_checkpoints(self):\n",
    "        \"\"\"\n",
    "        Clear the list of saved checkpoints.\n",
    "        \"\"\"\n",
    "```\n",
    "\n",
    "As you see in the next cell, we call both of these methods alternatively.\n",
    "We train N models, calling `add_checkpoint`, perform the active learning step and then restart by calling `clear_checkpoints`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "source": [
    "report = []\n",
    "for epoch in tqdm(range(hyperparams.epoch)):\n",
    "    model.clear_checkpoints()\n",
    "    # Load the initial weights.\n",
    "    for model_iter in range(hyperparams.iterations):\n",
    "        print(f\"Training model {model_iter}\")\n",
    "        model.load_state_dict(init_weights)\n",
    "        model.model.apply(weights_reset)\n",
    "        _ = model.train_on_dataset(active_set, optimizer=optimizer, batch_size=hyperparams.batch_size,\n",
    "                                 use_cuda=use_cuda, epoch=hyperparams.training_duration)\n",
    "        model.add_checkpoint()\n",
    "    \n",
    "    \n",
    "\n",
    "    # Get test NLL!\n",
    "    model.test_on_dataset(test_set, hyperparams.batch_size, use_cuda,\n",
    "                          average_predictions=hyperparams.iterations)\n",
    "    metrics = model.metrics\n",
    "\n",
    "    # We can now label the most uncertain samples according to our heuristic.\n",
    "    should_continue = active_loop.step()\n",
    "    # Keep track of progress\n",
    "    labelling_progress = active_set.labelled_map.astype(np.uint16)\n",
    "    if not should_continue:\n",
    "            break\n",
    "\n",
    "    test_loss = metrics['test_loss'].value\n",
    "    logs = {\n",
    "        \"test_nll\": test_loss,\n",
    "        \"epoch\": epoch,\n",
    "        \"Next Training set size\": len(active_set)\n",
    "    }\n",
    "    report.append(logs)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = [v['test_nll'] for v in report]\n",
    "y = [v['Next Training set size'] for v in report]\n",
    "plt.plot(y, x)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
