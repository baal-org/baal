{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Methods for approximating bayesian posteriors \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baal-org/baal/blob/master/notebooks/fundamentals/posteriors.ipynb)\n",
    "\n",
    "When we started developing active learning methods, we realised that what we wanted to\n",
    "achieve required estimating the uncertainty of models. Doing so for neural networks is\n",
    "an ongoing active research area.\n",
    "\n",
    "For the purposes of `baal`, we have implemented a few methods that are relatively generic\n",
    "and work with many neural networks.\n",
    "\n",
    "All the techniques implemented effectively produce approximate samples from the posterior.\n",
    "For classification techniques, this means that you usually end up with a 3D tensor rather\n",
    "than a 2D tensor (`n_batch x n_classes x n_samples` rather than `n_batch x n_classes`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Monte-Carlo Dropout\n",
    "\n",
    "Monte-Carlo Dropout, or MC Dropout, is a very simple way of accessing uncertainty\n",
    "in a network that include Dropout layers. Essentially, rather than turning off\n",
    "dropout during inference, you keep in on and make multiple predictions on the\n",
    "same data. Due to the stochastic zeroing of weights, you'll get a different for\n",
    "every iteration, even if the input is the same.\n",
    "\n",
    "This is valid primarily because you trained the network using dropout: You have\n",
    "already learnt to make predictions without all the weights.\n",
    "\n",
    "The output is a distribution of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Usage\n",
    "\n",
    "In order to use it, you can simply import Dropout layers from baal and use them in your model construction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "import baal.bayesian.dropout\n",
    "\n",
    "standard_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 8),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(8, 4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(4, 2),\n",
    ")\n",
    "\n",
    "mc_dropout_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 8),\n",
    "    torch.nn.ReLU(),\n",
    "    baal.bayesian.dropout.Dropout(p=0.5),\n",
    "    torch.nn.Linear(8, 4),\n",
    "    torch.nn.ReLU(),\n",
    "    baal.bayesian.dropout.Dropout(p=0.5),\n",
    "    torch.nn.Linear(4, 2),\n",
    ")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The main difference between these is that the standard model will set the dropout probability to zero during eval, while the MC dropout model will not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "dummy_input = torch.randn(8, 10)\n",
    "\n",
    "standard_model.eval()\n",
    "print(bool((standard_model(dummy_input) == standard_model(dummy_input)).all()))\n",
    "\n",
    "mc_dropout_model.eval()\n",
    "print(bool((mc_dropout_model(dummy_input) == mc_dropout_model(dummy_input)).all()))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In order to get a distribution of model outputs, you simply need to repeatedly run the same data through the MC Dropout model. `baal` makes this easier for you by providing a class called `ModelWrapper`. This class accepts your model and a criterion (loss) function, and provides several utility functions, such as running training steps and more. The one that is important for obtaining a posterior distribution is `Modelwrapper.predict_on_batch`.\n",
    "\n",
    "This method allows you to specify a number of iterations to run the model for, and produces a distribution accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from baal.modelwrapper import ModelWrapper\n",
    "\n",
    "wrapped_model = ModelWrapper(\n",
    "    mc_dropout_model,\n",
    "    torch.nn.MSELoss()\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = wrapped_model.predict_on_batch(dummy_input, iterations=10000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The tensor \"prediction_distribution\" has the shape (batch size) x (output size) x iterations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "predictions.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can visualise this posterior distribution, for example for the first data point in our\n",
    "minibatch (although note that because this model is overly simplistic, this is not very\n",
    "useful):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(predictions[0, 0, :].numpy(), bins=50);\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Drop Connect\n",
    "\n",
    "DropConnect is another way of accessing uncertainty\n",
    "in a network. The idea is very similar to MCdropout, however in Dropconnect the connection weights between layers are randomly chosen to be dropped. With mulpile pass through the network, similar to MCDropout we will endup with a distribution on the predictions.\n",
    "\n",
    "Eventually, the benefit of this approach is that you do not need to add a Dropout layer to a netwrok which doesn't have one. In long term using DropConnect is going to surpass MCDropout results in active learning but the draw backs is the essential need of almost twice iterations and longer trainings for the model to converge at each active learning step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Usage\n",
    "As usual we have pre-implemented wrappers to ease your job for this. Example below shows how to use this module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class DummyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DummyModel, self).__init__()\n",
    "        self.conv = torch.nn.Conv2d(3, 8, kernel_size=10)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.linear = torch.nn.Linear(8, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.linear(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import numpy as np\n",
    "from baal.bayesian import MCDropoutConnectModule\n",
    "from baal.modelwrapper import ModelWrapper\n",
    "\n",
    "dummy_model = DummyModel()\n",
    "dummy_input = torch.from_numpy(np.ones([3, 10, 10]) * 1 / 255.).float()\n",
    "model = MCDropoutConnectModule(dummy_model, layers=['Linear'], weight_dropout=0.9)\n",
    "wrapped_model = ModelWrapper(model, torch.nn.CrossEntropyLoss(), replicate_in_memory=False)\n",
    "with torch.no_grad():\n",
    "    predictions = wrapped_model.predict_on_batch(dummy_input.unsqueeze(0), iterations=10000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "predictions.shape"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "let's visualize the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(predictions[0, 0, :].numpy(), bins=50);\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As part of our experiments, we compare MCDropout(MCD) and DropConnect(MCDC). We let the experiments run for 2000 epochs on `vgg16` using `CIFAR10` and tried different number of iterations and weight drop rate for Dropconnect.\n",
    "Our experiments indicate that `DropConnect` could give a better result if it is used with number of iterations more than `80` and drop weight rate of around `50%`.\n",
    "\n",
    "The reference [paper](https://arxiv.org/pdf/1906.04569.pdf) indicates using a drop rate of `94%` give the best result but our experiments show otherwise.\n",
    "The main factor of change for DropConnect is the number of `iterations` used to estimate the posterior. However, as we can see for MCDropout, number of `iterations` 40 and 80 would give almost the same results.\n",
    " In order to prevent overfitting, we could change `learning rate` and use other techniques and get a lift on the performance, however as could be seen for higher `iterations`, DropConnect could easily outperform MCDropout at 10K training set size.\n",
    "\n",
    "Finally, the choice of method and training process is always up to the user and their current dataset.\n",
    "Parameters like how low the validation error should be and if the training is allowed to be run for few days or there is a time limit could all effect in which strategy is best and which hyperparameters to choose.\n",
    "\n",
    "<img src=\"https://github.com/baal-org/baal/blob/master/docs/literature/images/experiment_results/iterations_mcdc.png?raw=true\" width=\"25%\" height=\"25%\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
