{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Use BaaL in production (Classification)\n",
    "\n",
    "In this tutorial, we will show you how to use BaaL during your labeling task.\n",
    "\n",
    "**NOTE** In this tutorial, we assume that we do not know the labels!\n",
    "\n",
    "### Install baal\n",
    "\n",
    "```bash\n",
    "pip install baal\n",
    "```\n",
    "\n",
    "We will first need a dataset! For the purpose of this demo, we will use a classification dataset, but BaaL\n",
    "works on more than computer vision! As long as we can estimate the uncertainty of a prediction, BaaL can be used.\n",
    "\n",
    "We will use the [Natural Images Dataset](https://www.kaggle.com/prasunroy/natural-images).\n",
    "Please extract the data in `/tmp/natural_images`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 5174, Valid: 1725, Num. classes : 8\n"
     ]
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "files = glob('/tmp/natural_images/*/*.jpg')\n",
    "classes = os.listdir('/tmp/natural_images')\n",
    "train, test = train_test_split(files, random_state=1337)  # Split 75% train, 25% validation\n",
    "print(f\"Train: {len(train)}, Valid: {len(test)}, Num. classes : {len(classes)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Introducing `baal.active.FileDataset` and `baal.active.ActiveLearningDataset`\n",
    "\n",
    "FileDataset is simply an object that loads data and implements `def label(self, idx: int, lbl: Any)`.\n",
    "This methods is necessary to label items in the dataset. You can set any value you want for unlabelled items,\n",
    "in our example we use -1.\n",
    "\n",
    "`ActiveLearningDataset` is a wrapper around a `Dataset` that performs data management.\n",
    "When you iterate over it, it will return labelled items only.\n",
    "\n",
    "To learn more on dataset management, visit [this notebook](./fundamentals/active-learning.ipynb).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from baal.active import FileDataset, ActiveLearningDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize(224),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "# We use -1 to specify that the data is unlabeled.\n",
    "train_dataset = FileDataset(train, [-1] * len(train), train_transform)\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "# We use -1 to specify that the data is unlabeled.\n",
    "test_dataset = FileDataset(test, [-1] * len(test), test_transform)\n",
    "active_learning_ds = ActiveLearningDataset(train_dataset, pool_specifics={'transform': test_transform})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "We now have two unlabeled datasets : train and validation. We encapsulate the training dataset in a \n",
    "`ActiveLearningDataset` object which will take care of the split between labeled and unlabeled samples.\n",
    "We are now ready to use Active Learning.\n",
    "We will use a technique called MC-Dropout, BaaL supports other techniques (see README) and proposes a similar API\n",
    "for each of them.\n",
    "When using MC-Dropout with BaaL, you can use any model as long as there are some Dropout Layers. These layers are essential to compute\n",
    "the uncertainty of the model.\n",
    "\n",
    "BaaL propose several models, but it also supports custom models using baal.bayesian.dropout.MCDropoutModule.\n",
    "\n",
    "In this example, we will use VGG-16, a popular model from `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from baal.modelwrapper import ModelWrapper\n",
    "from torchvision.models import vgg16\n",
    "from baal.bayesian.dropout import MCDropoutModule\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "model = vgg16(pretrained=False, num_classes=len(classes))\n",
    "# This will modify all Dropout layers to be usable at test time which is\n",
    "# required to perform Active Learning.\n",
    "model = MCDropoutModule(model)\n",
    "if USE_CUDA:\n",
    "  model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# ModelWrapper is an object similar to keras.Model.\n",
    "baal_model = ModelWrapper(model, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Heuristics\n",
    "\n",
    "To rank uncertainty, we will use a heuristic. For classification and segmentation, BALD is the recommended\n",
    "heuristic. We will also add noise to the heuristic to lower the selection bias added by the AL process.\n",
    "This is done by specifying `shuffle_prop` in the heuristic constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from baal.active.heuristics import BALD\n",
    "heuristic = BALD(shuffle_prop=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Oracle\n",
    "When the AL process requires a new item to labeled, we need to provide an Oracle. In your case, the Oracle will\n",
    "be a human labeler most likely. For this example, we're lucky the class label is in the image path!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This function would do the work that a human would do.\n",
    "def get_label(img_path):\n",
    "  return classes.index(img_path.split('/')[-2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Labeling process\n",
    "The labeling will go like this:\n",
    "1. Label all the test set and some samples from the training set.\n",
    "2. Train the model for a few epoch on the training set.\n",
    "3. Select the K-top uncertain samples according to the heuristic.\n",
    "4. Label those samples.\n",
    "5. If not done, go back to 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. labeled: 100/5174\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1. Label all the test set and some samples from the training set.\n",
    "for idx in range(len(test_dataset)):\n",
    "  img_path = test_dataset.files[idx]\n",
    "  test_dataset.label(idx, get_label(img_path))\n",
    "  \n",
    "# Let's label 100 training examples randomly first.\n",
    "# Note: the indices here are relative to the pool of unlabelled items!\n",
    "train_idxs = np.random.permutation(np.arange(len(train_dataset)))[:100].tolist()\n",
    "labels = [get_label(train_dataset.files[idx]) for idx in train_idxs]\n",
    "active_learning_ds.label(train_idxs, labels)\n",
    "\n",
    "print(f\"Num. labeled: {len(active_learning_ds)}/{len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:109] 2021-07-28T14:47:48.133213Z [\u001B[32minfo     ] Starting training              dataset=100 epoch=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torch/utils/data/dataloader.py:478: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/opt/conda/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:119] 2021-07-28T14:48:07.477011Z [\u001B[32minfo     ] Training complete              train_loss=2.058176279067993\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:147] 2021-07-28T14:48:07.479793Z [\u001B[32minfo     ] Starting evaluating            dataset=1725\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:156] 2021-07-28T14:48:21.277716Z [\u001B[32minfo     ] Evaluation complete            test_loss=2.0671451091766357\n",
      "Metrics: {'test_loss': 2.0671451091766357, 'train_loss': 2.058176279067993}\n"
     ]
    }
   ],
   "source": [
    "# 2. Train the model for a few epoch on the training set.\n",
    "baal_model.train_on_dataset(active_learning_ds, optimizer, batch_size=16, epoch=5, use_cuda=USE_CUDA)\n",
    "baal_model.test_on_dataset(test_dataset, batch_size=16, use_cuda=USE_CUDA)\n",
    "\n",
    "print(\"Metrics:\", {k:v.avg for k,v in baal_model.metrics.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[103-MainThread  ] [baal.modelwrapper:predict_on_dataset_generator:241] 2021-07-28T14:48:21.291851Z [\u001B[32minfo     ] Start Predict                  dataset=5074\n"
     ]
    }
   ],
   "source": [
    "# 3. Select the K-top uncertain samples according to the heuristic.\n",
    "pool = active_learning_ds.pool\n",
    "if len(pool) == 0:\n",
    "  raise ValueError(\"We're done!\")\n",
    "\n",
    "# We make 15 MCDropout iterations to approximate the uncertainty.\n",
    "predictions = baal_model.predict_on_dataset(pool, batch_size=16, iterations=15, use_cuda=USE_CUDA, verbose=False)\n",
    "# We will label the 10 most uncertain samples.\n",
    "top_uncertainty = heuristic(predictions)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 1429), (4, 2971), (2, 1309), (4, 5), (3, 3761), (4, 2708), (6, 4679), (7, 160), (7, 1638), (6, 73)]\n"
     ]
    }
   ],
   "source": [
    "# 4. Label those samples.\n",
    "oracle_indices = active_learning_ds._pool_to_oracle_index(top_uncertainty)\n",
    "labels = [get_label(train_dataset.files[idx]) for idx in oracle_indices]\n",
    "print(list(zip(labels, oracle_indices)))\n",
    "active_learning_ds.label(top_uncertainty, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 110 items!\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:109] 2021-07-28T14:50:02.089160Z [\u001B[32minfo     ] Starting training              dataset=110 epoch=5\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:119] 2021-07-28T14:50:19.678241Z [\u001B[32minfo     ] Training complete              train_loss=1.9793428182601929\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:147] 2021-07-28T14:50:19.681509Z [\u001B[32minfo     ] Starting evaluating            dataset=1725\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:156] 2021-07-28T14:50:33.777658Z [\u001B[32minfo     ] Evaluation complete            test_loss=2.013453960418701\n",
      "Metrics: {'test_loss': 2.013453960418701, 'train_loss': 1.9793428182601929}\n",
      "[103-MainThread  ] [baal.modelwrapper:predict_on_dataset_generator:241] 2021-07-28T14:50:33.784990Z [\u001B[32minfo     ] Start Predict                  dataset=5064\n",
      "Training on 120 items!\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:109] 2021-07-28T14:52:14.295969Z [\u001B[32minfo     ] Starting training              dataset=120 epoch=5\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:119] 2021-07-28T14:52:32.482238Z [\u001B[32minfo     ] Training complete              train_loss=1.8900309801101685\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:147] 2021-07-28T14:52:32.484473Z [\u001B[32minfo     ] Starting evaluating            dataset=1725\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:156] 2021-07-28T14:52:46.287436Z [\u001B[32minfo     ] Evaluation complete            test_loss=1.8315811157226562\n",
      "Metrics: {'test_loss': 1.8315811157226562, 'train_loss': 1.8900309801101685}\n",
      "[103-MainThread  ] [baal.modelwrapper:predict_on_dataset_generator:241] 2021-07-28T14:52:46.367016Z [\u001B[32minfo     ] Start Predict                  dataset=5054\n",
      "Training on 130 items!\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:109] 2021-07-28T14:54:26.794349Z [\u001B[32minfo     ] Starting training              dataset=130 epoch=5\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:119] 2021-07-28T14:54:44.481490Z [\u001B[32minfo     ] Training complete              train_loss=1.961772084236145\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:147] 2021-07-28T14:54:44.483477Z [\u001B[32minfo     ] Starting evaluating            dataset=1725\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:156] 2021-07-28T14:54:58.268424Z [\u001B[32minfo     ] Evaluation complete            test_loss=1.859472393989563\n",
      "Metrics: {'test_loss': 1.859472393989563, 'train_loss': 1.961772084236145}\n",
      "[103-MainThread  ] [baal.modelwrapper:predict_on_dataset_generator:241] 2021-07-28T14:54:58.276565Z [\u001B[32minfo     ] Start Predict                  dataset=5044\n",
      "Training on 140 items!\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:109] 2021-07-28T14:56:38.406344Z [\u001B[32minfo     ] Starting training              dataset=140 epoch=5\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:119] 2021-07-28T14:56:57.088064Z [\u001B[32minfo     ] Training complete              train_loss=1.8688158988952637\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:147] 2021-07-28T14:56:57.091358Z [\u001B[32minfo     ] Starting evaluating            dataset=1725\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:156] 2021-07-28T14:57:10.968456Z [\u001B[32minfo     ] Evaluation complete            test_loss=1.7242822647094727\n",
      "Metrics: {'test_loss': 1.7242822647094727, 'train_loss': 1.8688158988952637}\n",
      "[103-MainThread  ] [baal.modelwrapper:predict_on_dataset_generator:241] 2021-07-28T14:57:10.977104Z [\u001B[32minfo     ] Start Predict                  dataset=5034\n",
      "Training on 150 items!\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:109] 2021-07-28T14:58:51.197386Z [\u001B[32minfo     ] Starting training              dataset=150 epoch=5\n",
      "[103-MainThread  ] [baal.modelwrapper:train_on_dataset:119] 2021-07-28T14:59:09.779341Z [\u001B[32minfo     ] Training complete              train_loss=1.8381125926971436\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:147] 2021-07-28T14:59:09.782580Z [\u001B[32minfo     ] Starting evaluating            dataset=1725\n",
      "[103-MainThread  ] [baal.modelwrapper:test_on_dataset:156] 2021-07-28T14:59:23.176680Z [\u001B[32minfo     ] Evaluation complete            test_loss=1.7318601608276367\n",
      "Metrics: {'test_loss': 1.7318601608276367, 'train_loss': 1.8381125926971436}\n",
      "[103-MainThread  ] [baal.modelwrapper:predict_on_dataset_generator:241] 2021-07-28T14:59:23.184444Z [\u001B[32minfo     ] Start Predict                  dataset=5024\n"
     ]
    }
   ],
   "source": [
    "# 5. If not done, go back to 2.\n",
    "for step in range(5): # 5 Active Learning step!\n",
    "  # 2. Train the model for a few epoch on the training set.\n",
    "  print(f\"Training on {len(active_learning_ds)} items!\")\n",
    "  baal_model.train_on_dataset(active_learning_ds, optimizer, batch_size=16, epoch=5, use_cuda=USE_CUDA)\n",
    "  baal_model.test_on_dataset(test_dataset, batch_size=16, use_cuda=USE_CUDA)\n",
    "\n",
    "  print(\"Metrics:\", {k:v.avg for k,v in baal_model.metrics.items()})\n",
    "  \n",
    "  # 3. Select the K-top uncertain samples according to the heuristic.\n",
    "  pool = active_learning_ds.pool\n",
    "  if len(pool) == 0:\n",
    "    print(\"We're done!\")\n",
    "    break\n",
    "  predictions = baal_model.predict_on_dataset(pool, batch_size=16, iterations=15, use_cuda=USE_CUDA, verbose=False)\n",
    "  top_uncertainty = heuristic(predictions)[:10]\n",
    "  # 4. Label those samples.\n",
    "  oracle_indices = active_learning_ds._pool_to_oracle_index(top_uncertainty)\n",
    "  labels = [get_label(train_dataset.files[idx]) for idx in oracle_indices]\n",
    "  active_learning_ds.label(top_uncertainty, labels)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And we're done!\n",
    "Be sure to save the dataset and the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "  'active_dataset': active_learning_ds.state_dict(),\n",
    "  'model': baal_model.state_dict(),\n",
    "  'metrics': {k:v.avg for k,v in baal_model.metrics.items()}\n",
    "}, '/tmp/baal_output.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Support\n",
    "Submit an issue or reach us to our Gitter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}