{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Can active learning preemptively mitigate fairness issues?\n",
    "*By Parmida Atighehchian*\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/baal-org/baal/blob/master/notebooks/fairness/ActiveFairness.ipynb)\n",
    "\n",
    "The purpose of this notebook is to demonstrate the prilimary results of our recent [contribution](https://arxiv.org/abs/2104.06879) to ICLR workshop of Responsible AI 2021.\n",
    "We show that active learning could help in creating fairer datasets without the need to know the bias in the dataset. This is important since in real scenarios, the source of bias is often unknown. Using active learning (i.e. BALD), we show that the prior knowledge of the bias is not necessary and hence it could be easier to integrate this setup in pipelines to make sure that the dataset is generally fairer and the possible biases are reduced. \n",
    "\n",
    "For the purpose of this demo, we use [Synbols](https://github.com/ElementAI/synbols) dataset. Synbols is the new state of the art generating synthetic datasets.\n",
    "\n",
    "The Dockerfile is located at `baal/notebooks/fairness/Docker_biased_data`.\n",
    "\n",
    "More resources on Baal:\n",
    "\n",
    "* [Literature review](https://baal.readthedocs.io/en/latest/literature/index.html)\n",
    "* [Active learning dataset and training loop classes](https://baal.readthedocs.io/en/latest/notebooks/fundamentals/active-learning.html)\n",
    "* [Methods for approximating bayesian posteriors](https://baal.readthedocs.io/en/latest/notebooks/fundamentals/posteriors.html)\n",
    "* [Full active learning example](https://baal.readthedocs.io/en/latest/notebooks/active_learning_process.html)\n",
    "\n",
    "\n",
    "If you have any question, please submit an issue or reach out on Gitter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Introducing bias in dataset\n",
    "\n",
    "Using Synbols, we will generate a character classification dataset with an important correlation between the character and the color.\n",
    "There is a correlation between the color blue and the character a:\n",
    "\n",
    "$p(char=a | color=blue) = 90\\%$\n",
    "\n",
    "and there is a correlation between the color red and the character d:\n",
    "\n",
    "$p(char=d | color=red) = 90\\%$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import numpy as np\n",
    "from math import pi\n",
    "from synbols.data_io import pack_dataset\n",
    "from synbols import drawing\n",
    "from synbols import generate\n",
    "\n",
    "class InfoSolid(drawing.SolidColor):\n",
    "    def attribute_dict(self):\n",
    "        d = super().attribute_dict()\n",
    "        d['color'] = self.color\n",
    "        return d\n",
    "    \n",
    "rng = np.random.RandomState(1337)\n",
    "p = .1\n",
    "blue = (0,0,255)\n",
    "red = (255, 0, 0)\n",
    "\n",
    "class SpuriousSampler:\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        \n",
    "    def __call__(self, seed):\n",
    "        \"\"\"Makes color dependant on symbol.\"\"\"\n",
    "        rng = np.random.RandomState(seed)\n",
    "        color = [blue, red][rng.choice([0, 1], p=[self.p, 1-self.p])]\n",
    "        char = rng.choice(['a', 'd'])\n",
    "        color_p = {'a':self.p, 'd':1-self.p}[char]\n",
    "        color = [blue, red][rng.choice([0, 1], p=[color_p, 1-color_p])]\n",
    "\n",
    "        fg = InfoSolid(color)\n",
    "        fg.color = color\n",
    "\n",
    "        attr_sampler = generate.basic_attribute_sampler(\n",
    "            char=char, foreground=fg, background=None, inverse_color=False, resolution=(64, 64))\n",
    "        d = attr_sampler()\n",
    "        return d\n",
    "\n",
    "\n",
    "def make_dataset(p, seed, num):\n",
    "    attribute_sampler = SpuriousSampler(p=p)\n",
    "    x, mask, y = pack_dataset(generate.dataset_generator(attribute_sampler, num, generate.flatten_mask, dataset_seed=seed))\n",
    "    for yi in y:\n",
    "        yi['color'] = 'red' if yi['foreground']['color'] == red else 'blue'\n",
    "    return (x,y,y)             \n",
    "\n",
    "train_set = make_dataset(p=0.9, seed=1000, num=10000)\n",
    "test_set = make_dataset(p=0.5, seed=2000, num=5000)\n",
    "dataset = {'train': train_set, 'test': test_set}"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Prepare model and dataset to be used in Baal setup\n",
    "As usual we wrap the train_set in `ActiveLearningDataset` and using vgg16 as default model, we use the Baal's `patch_module` to create a dropout layer which performs in inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from torchvision.transforms import transforms\n",
    "from active_fairness.dataset import SynbolDataset\n",
    "from baal.active import get_heuristic, ActiveLearningDataset\n",
    "from typing import Dict\n",
    "\n",
    "IMG_SIZE=64\n",
    "\n",
    "def get_datasets(dataset : Dict, initial_pool: int, attribute:str, target_key:str):\n",
    "    \"\"\"\n",
    "    Get the dataset for the experiment.\n",
    "    Args:\n",
    "        dataset: The synbol generated dataset.\n",
    "        initial_pool: Initial number of items to label.\n",
    "        attribute: Key where the sensitive attribute is.\n",
    "        target_key: Key where the target is.\n",
    "    Returns:\n",
    "        ActiveLearningDataset with `initial_pool` items labelled\n",
    "        Test dataset\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose(\n",
    "        [transforms.ToPILImage(),\n",
    "         transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "         transforms.RandomHorizontalFlip(),\n",
    "         transforms.RandomRotation(30),\n",
    "         transforms.ToTensor(),\n",
    "         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "    test_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                         transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
    "                                                              (0.2023, 0.1994, 0.2010))])\n",
    "    train_ds = dataset['train']\n",
    "    test_ds = dataset['test']\n",
    "    ds = SynbolDataset(*train_ds, target_key=target_key, attribute=attribute,\n",
    "                           transform=transform)\n",
    "\n",
    "    test_set = SynbolDataset(*test_ds, target_key=target_key, attribute=attribute,\n",
    "                                 transform=test_transform)\n",
    "\n",
    "    active_set = ActiveLearningDataset(ds, pool_specifics={'transform': test_transform})\n",
    "    active_set.label_randomly(initial_pool)\n",
    "    return active_set, test_set"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from torchvision import models\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from baal.bayesian.dropout import patch_module\n",
    "\n",
    "#set use_cuda to False if you don't have access to GPUS\n",
    "use_cuda=True\n",
    "\n",
    "model = models.vgg16(pretrained=False, num_classes=2)\n",
    "weights = load_state_dict_from_url('https://download.pytorch.org/models/vgg16-397923af.pth')\n",
    "weights = {k: v for k, v in weights.items() if 'classifier.6' not in k}\n",
    "model.load_state_dict(weights, strict=False)\n",
    "\n",
    "# change dropout layer to MCDropout\n",
    "model = patch_module(model)\n",
    "\n",
    "if use_cuda:\n",
    "    model.cuda()\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We wrap the pytorch criterion to accomodate target being a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from torch import nn\n",
    "\n",
    "class Criterion(nn.Module):\n",
    "    def __init__(self, crit):\n",
    "        super().__init__()\n",
    "        self.crit = crit\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return self.crit(input, target['target'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Training\n",
    "\n",
    "Let's now train the model with active learning. As usual, we compare `bald` with `random` but this time, we are looking for something else in the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "no_output"
    ],
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from baal.modelwrapper import ModelWrapper\n",
    "from baal.active.heuristics import BALD\n",
    "from baal.active.active_loop import ActiveLearningLoop\n",
    "from active_fairness.metrics import FairnessMetric\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "heuristics = ['bald', 'random']\n",
    "\n",
    "logs = {'bald': {}, 'random': {}}\n",
    "\n",
    "for heuristic_name in heuristics:\n",
    "    active_set, test_set = get_datasets(dataset, initial_pool=500, attribute='color', target_key='char')\n",
    "\n",
    "    heuristic = get_heuristic(name=heuristic_name, shuffle_prop=0.0)\n",
    "\n",
    "    criterion = Criterion(CrossEntropyLoss())\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "    wrapped_model = ModelWrapper(model, criterion)\n",
    "\n",
    "    wrapped_model.add_metric('aggregate_res', lambda: FairnessMetric(skm.accuracy_score, name='acc',\n",
    "                                                                     attribute='color'))\n",
    "\n",
    "    # save imagenet weights\n",
    "    init_weights = deepcopy(model.state_dict())\n",
    "\n",
    "\n",
    "    bald = BALD()\n",
    "\n",
    "\n",
    "\n",
    "    # for prediction we use a smaller batchsize\n",
    "    # since it is slower\n",
    "    active_loop = ActiveLearningLoop(active_set,\n",
    "                                     wrapped_model.predict_on_dataset,\n",
    "                                     heuristic,\n",
    "                                     50,\n",
    "                                     batch_size=16,\n",
    "                                     iterations=20,\n",
    "                                     use_cuda=use_cuda,\n",
    "                                     workers=0)\n",
    "    learning_epoch = 20\n",
    "    for epoch in tqdm(range(100000)):\n",
    "        wrapped_model.load_state_dict(init_weights)\n",
    "        wrapped_model.train_on_dataset(active_set, optimizer, batch_size=32,\n",
    "                                       epoch=learning_epoch, use_cuda=True, workers=12)\n",
    "\n",
    "        # Validation!\n",
    "        wrapped_model.test_on_dataset(test_set, batch_size=32, use_cuda=use_cuda,\n",
    "                                      workers=12, average_predictions=20)\n",
    "\n",
    "        should_continue = active_loop.step()\n",
    "        if not should_continue:\n",
    "            break\n",
    "\n",
    "        # Send logs\n",
    "        fair_train = wrapped_model.metrics[f'train_aggregate_res'].value\n",
    "        epoch_logs = {\n",
    "            'epoch': epoch,\n",
    "            'test_loss': wrapped_model.metrics['test_loss'].value,\n",
    "            'active_train_size': len(active_set)}\n",
    "\n",
    "        agg_res = {'train_' + k: v for k, v in fair_train.items()}\n",
    "        epoch_logs.update(agg_res)\n",
    "\n",
    "        for k, v in epoch_logs.items():\n",
    "            if k in logs[heuristic_name].keys():\n",
    "                logs[heuristic_name][k].append(v)\n",
    "            else:\n",
    "                logs[heuristic_name][k] = [v]\n",
    "\n",
    "        if len(active_set) > 2000:\n",
    "            break"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Results and Discussion\n",
    "\n",
    "Below we show the number of samples added to each subcategory (i.e. character with a specific color) as the training goes on. Interesting result is that the number of samples added to the minority group of each character increases using `bald` where as `random` picks samples in a random setup and hence having more samples given a protected attribute (here color), random has more samples of a certain color to pick. This indicates that active learning with `bald` generally leads to a more fair dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "x = logs['bald']['epoch']\n",
    "fig, ((ax0, ax1), (ax2, ax3)) = plt.subplots(nrows=2, ncols=2, sharex=True,\n",
    "                                    figsize=(12, 6))\n",
    "plots_target = [('minority count for character a', 'train_count_0_red'),\n",
    "                 ('minority count for character b', 'train_count_1_blue'),\n",
    "               ('majority count for character a', 'train_count_0_blue'),\n",
    "               ('majority count for character b', 'train_count_1_red')]\n",
    "\n",
    "for ax, (title, key) in zip([ax0, ax1, ax2, ax3], plots_target):\n",
    "    ax.set_title(title)\n",
    "    ax.plot(x, logs['bald'][key], color='r', label=\"BALD\")\n",
    "    ax.plot(x, logs['random'][key], color='b', label=\"Uniform\")\n",
    "    ax.set_xlabel('Active step')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.legend()\n",
    "\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We demonstrate the `test_loss` and `training_size` using `bald` vs `random` as heuristics. As it is shown, the trainig size increases with the same pace but the above graphs shows the underlying difference in the existing samples for each class which then results in also a better loss decrease using `bald`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "x = logs['bald']['epoch']\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=1, ncols=2, sharex=True,\n",
    "                                    figsize=(12, 6))\n",
    "ax0.set_title('training size')\n",
    "ax0.plot(x, logs['bald']['active_train_size'], color='r', label='BALD')\n",
    "ax0.plot(x, logs['random']['active_train_size'], color='b', label='Uniform')\n",
    "\n",
    "ax1.set_title('test loss')\n",
    "ax1.plot(x, logs['bald']['test_loss'], color='r', label='BALD')\n",
    "ax1.plot(x, logs['random']['test_loss'], color='b', label='Uniform')\n",
    "ax1.legend()\n",
    "fig.show()"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
