{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Speeding up Monte-Carlo Inference With MCCachingModule\n",
    "\n",
    "It is common knowledge that running MCDropout is slow and computationally expensive.\n",
    "Baal proposes a new simple API called `MCCachingModule` to speedup MCDropout by more than 70%!\n",
    "\n",
    "**TLDR: MCCachingWrapper**\n",
    "\n",
    "```python\n",
    ">>> from baal.bayesian.caching_utils import MCCachingModule\n",
    ">>> # Regular code to perform MCDropout with Baal.\n",
    ">>> model = MCDropoutModule(original_module)\n",
    ">>> # To gain 70% speedup, simply do\n",
    ">>> model = MCCachingModule(model)\n",
    "```\n",
    "\n",
    "Below we detail our approach in this toy example. We will use a `VGG16` model and run MCDropout for 20 iterations on the test set of CIFAR10.\n",
    "\n",
    "We get the following results on a GeForce 1060Ti:\n",
    "\n",
    "| Number of Iteration | 20       | 50       | 100      |\n",
    "|---------------------|----------|----------|----------|\n",
    "| Regular MC-Dropout  | 2:58     | 7:27     | 13:45    |\n",
    "| Ours                | **0:50** | **1:46** | **3:32** |\n",
    "\n",
    "We are excited to see how the community uses this new feature!\n",
    "\n",
    "### Code!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.models import vgg16\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "from baal.bayesian.caching_utils import MCCachingModule\n",
    "from baal.bayesian.dropout import MCDropoutModule\n",
    "from baal.modelwrapper import ModelWrapper\n",
    "\n",
    "ITERATIONS = 20\n",
    "\n",
    "vgg = vgg16().cuda()\n",
    "vgg.eval()\n",
    "\n",
    "ds = CIFAR10('/tmp', train=False, transform=ToTensor(), download=True)\n",
    "\n",
    "# Takes ~2:58 minutes.\n",
    "with MCDropoutModule(vgg) as model_2:\n",
    "    wrapper = ModelWrapper(model_2, None, replicate_in_memory=False)\n",
    "    wrapper.predict_on_dataset(ds, batch_size=32, iterations=ITERATIONS, use_cuda=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T21:12:23.378811603Z",
     "start_time": "2023-07-13T21:09:29.068365127Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Introducing MCCachingModule!\n",
    "\n",
    "By simply wrapping the module with `MCCachingModule` we run the same inference 70% faster!\n",
    "\n",
    "**NOTE**: You should *always* use `ModelWrapper(..., replicate_in_memory=False)` when in combination with `MCCachingModule`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Takes ~50 seconds!.\n",
    "with MCCachingModule(vgg) as model:\n",
    "    with MCDropoutModule(model) as model_2:\n",
    "        wrapper = ModelWrapper(model_2, None, replicate_in_memory=False)\n",
    "        wrapper.predict_on_dataset(ds, batch_size=32, iterations=ITERATIONS, use_cuda=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-07-13T21:13:11.076629413Z",
     "start_time": "2023-07-13T21:12:23.387507076Z"
    }
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
