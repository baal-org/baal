{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using stochastic weight averaging\n",
    "\n",
    "Stochastic weight averaging with gaussian modelling (SWAG) is a method to approximate posteriors by sampling\n",
    "the weights of a neural network from a gaussian distribution that is fitted to samples from the stochastic\n",
    "gradient descent iterates.\n",
    "\n",
    "We implement this in `baal` as an optimiser, since the samples are taken during the optimisation steps.\n",
    "\n",
    "This optimiser class is in the `swag` submodule. It accepts all parameters as the standard SGD optimiser\n",
    "in pytorch, and in addition accepts three parameters to determine the SWAG behaviour:\n",
    "\n",
    "- `swa_burn_in`, or how many steps of SGD optimisation are taken before SWAG samples are collected\n",
    "- `swa_steps`, or how many steps (i.e. mini-batches) to take in between SWAG samples\n",
    "- `n_deviations`, or how many of the most recent deviations to use to fit the gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal import swag\n",
    "\n",
    "optimiser = swag.StochasticWeightAveraging(\n",
    "    standard_model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    swa_burn_in=100,\n",
    "    swa_steps=20,\n",
    "    n_deviations=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you use this optimiser to fit your neural network, the optimiser will collect the mean and\n",
    "variation of each of the network's weights every 20 SGD steps, following the first 100 SGD steps.\n",
    "\n",
    "_Usually_, you should set `swa_burn_in` to upwards of tens of epochs, and swa_steps to on the\n",
    "order of one epoch. Note that the optimiser is unaware of the length of an epoch, and so you\n",
    "need to specify the amounts in terms of SGD steps, or mini-batches.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baal import swag\n",
    "import torch\n",
    "import torch.utils.data\n",
    "\n",
    "# the data shape:\n",
    "batch_size = 8\n",
    "dataset_size = 20 * 8\n",
    "# create some dummy data:\n",
    "x = torch.randn(dataset_size, 10)\n",
    "y = torch.randint(low=0, high=2, size=(dataset_size,)).long()\n",
    "# write loaders for these:\n",
    "dummy_dataset = torch.utils.data.TensorDataset(x, y)\n",
    "dummy_loader = torch.utils.data.DataLoader(dummy_dataset, batch_size=batch_size)\n",
    "# write a simple model:\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(10, 5),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Dropout(p=0.5),\n",
    "    torch.nn.Linear(5, 2),\n",
    ")\n",
    "criterion = torch.nn.NLLLoss()\n",
    "# create the SWAG optimiser:\n",
    "optimiser = swag.StochasticWeightAveraging(\n",
    "    model.parameters(),\n",
    "    lr=1e-3,\n",
    "    weight_decay=1e-5,\n",
    "    # burn in for 50 epochs:\n",
    "    swa_burn_in=50*len(dummy_loader),\n",
    "    # then collect samples every epoch:\n",
    "    swa_steps=len(dummy_loader),\n",
    "    n_deviations=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then train our model as normal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(75):\n",
    "\n",
    "    for x, y in dummy_loader:\n",
    "\n",
    "        loss = criterion(model(x), y)\n",
    "        loss.backward()\n",
    "        optimiser.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain uncertainty estimates for our predictions, we need to \"sample\" models\n",
    "multiple times and make predictions with each model separately.\n",
    "\n",
    "For example, if we want to sample from the approximate posterior 100 times we do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_batch = torch.randn(8, 10)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for n in range(1000):\n",
    "        optimiser.sample()\n",
    "        prediction = model(eval_batch)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "predictions = torch.stack(predictions, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this can be simplified by using the `swag.ModelWrapper` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_wrapper = swag.SwagModelWrapper(\n",
    "    model,\n",
    "    criterion\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    predictions = model_wrapper.predict_on_batch(\n",
    "        eval_batch,\n",
    "        optimiser,\n",
    "        iterations=1000\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 1000 predictions for every data point in our mini-batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualise the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(predictions[0, 0, :].numpy(), bins=50);\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
