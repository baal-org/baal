{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Use BaaL in production (Classification)\n",
    "\n",
    "In this tutorial, we will show you how to use BaaL during your labeling task.\n",
    "\n",
    "### Install baal\n",
    "\n",
    "```bash\n",
    "pip install baal\n",
    "```\n",
    "\n",
    "We will first need a dataset! For the purpose of this demo, we will use a classification dataset, but BaaL\n",
    "works on segmentation, classification and regression.\n",
    "\n",
    "We will use the [Natural Images Dataset](https://www.kaggle.com/prasunroy/natural-images).\n",
    "Please extract the data in `/tmp/natural_images`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Train: 5174, Valid: 1725, Num. classes : 8\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "files = glob('/tmp/natural_images/*/*.jpg')\n",
    "classes = os.listdir('/tmp/natural_images')\n",
    "train, test = train_test_split(files, random_state=1337)  # Split 75% train, 25% validation\n",
    "print(f\"Train: {len(train)}, Valid: {len(test)}, Num. classes : {len(classes)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from baal.active import FileDataset, ActiveLearningDataset\n",
    "from torchvision import transforms\n",
    "\n",
    "train_transform = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.Resize(224),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "# We use -1 to specify that the data is unlabeled.\n",
    "train_dataset = FileDataset(train, [-1] * len(train), train_transform)\n",
    "\n",
    "test_transform = transforms.Compose([transforms.Resize(224),\n",
    "                                      transforms.RandomCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
    "\n",
    "# We use -1 to specify that the data is unlabeled.\n",
    "test_dataset = FileDataset(test, [-1] * len(test), test_transform)\n",
    "active_learning_ds = ActiveLearningDataset(train_dataset, pool_specifics={'transform': test_transform})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "We now have two unlabeled datasets : train and validation. We encapsulate the training dataset in a \n",
    "`ActiveLearningDataset` object which will take care of the split between labeled and unlabeled samples.\n",
    "We are now ready to use Active Learning.\n",
    "We will use a technique called MC-Dropout, BaaL supports other techniques (see README) and proposes a similar API\n",
    "for each of them.\n",
    "When using MC-Dropout with BaaL, you can use any model as long as there are some Dropout Layers. These layers are essential to compute\n",
    "the uncertainty of the model.\n",
    "\n",
    "BaaL propose several models, but it also supports custom models using baal.bayesian.dropout.MCDropoutModule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from baal.modelwrapper import ModelWrapper\n",
    "from torchvision.models import vgg16\n",
    "from baal.bayesian.dropout import MCDropoutModule\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "model = vgg16(pretrained=False, num_classes=len(classes))\n",
    "# This will modify all Dropout layers to be usable at test time which is\n",
    "# required to perform Active Learning.\n",
    "model = MCDropoutModule(model)\n",
    "if USE_CUDA:\n",
    "  model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "# ModelWrapper is an object similar to keras.Model.\n",
    "baal_model = ModelWrapper(model, criterion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Heuristics\n",
    "\n",
    "To rank uncertainty, we will use a heuristic. For classification and segmentation, BALD is the recommended\n",
    "heuristic. We will also add noise to the heuristic to lower the selection bias added by the AL process.\n",
    "This is done by specifying `shuffle_prop` in the heuristic constructor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from baal.active.heuristics import BALD\n",
    "heuristic = BALD(shuffle_prop=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Oracle\n",
    "When the AL process requires a new item to labeled, we need to provide an Oracle. In your case, the Oracle will\n",
    "be a human labeler most likely. For this example, we're lucky the class label is in the image path!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_label(img_path):\n",
    "  return classes.index(img_path.split('/')[-2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Labeling process\n",
    "The labeling will go like this:\n",
    "1. Label all the test set and some samples from the training set.\n",
    "2. Train the model for a few epoch on the training set.\n",
    "3. Select the K-top uncertain samples according to the heuristic.\n",
    "4. Label those samples.\n",
    "5. If not done, go back to 2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. labeled: 100/5174\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# 1. Label all the test set and some samples from the training set.\n",
    "for idx in range(len(test_dataset)):\n",
    "  img_path = test_dataset.files[idx]\n",
    "  test_dataset.label(idx, get_label(img_path))\n",
    "  \n",
    "# Let's label 100 training examples randomly first.\n",
    "# Note: the indices here are relative to the pool!\n",
    "train_idxs = np.random.permutation(np.arange(len(train_dataset)))[:100].tolist()\n",
    "labels = [get_label(train_dataset.files[idx]) for idx in train_idxs]\n",
    "active_learning_ds.label(train_idxs, labels)\n",
    "\n",
    "print(f\"Num. labeled: {len(active_learning_ds)}/{len(train_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:45] 2019-06-26T20:52:15.048304Z [\u001b[32minfo     ] Starting training              dataset=100 epoch=5\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:54] 2019-06-26T20:52:26.082985Z [\u001b[32minfo     ] Training complete              train_loss=0.4015560448169708\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:80] 2019-06-26T20:52:26.088695Z [\u001b[32minfo     ] Starting evaluating            dataset=1725\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:87] 2019-06-26T20:52:43.093968Z [\u001b[32minfo     ] Evaluation complete            test_loss=0.4138002097606659\n",
      "Metrics: {'train': 0.4015560448169708, 'test': 0.4138002097606659}\n"
     ]
    }
   ],
   "source": [
    "# 2. Train the model for a few epoch on the training set.\n",
    "baal_model.train_on_dataset(active_learning_ds, optimizer, batch_size=16, epoch=5, use_cuda=USE_CUDA)\n",
    "baal_model.test_on_dataset(test_dataset, batch_size=16, use_cuda=USE_CUDA)\n",
    "\n",
    "print(\"Metrics:\", {k:v.avg for k,v in baal_model.metrics.items()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14-MainThread   ] [baal.modelwrapper:predict_on_dataset_generator:113] 2019-06-26T20:52:43.179018Z [\u001b[32minfo     ] Start Predict                  dataset=5074\n"
     ]
    }
   ],
   "source": [
    "# 3. Select the K-top uncertain samples according to the heuristic.\n",
    "pool = active_learning_ds.pool\n",
    "if len(pool) == 0:\n",
    "  raise ValueError(\"We're done!\")\n",
    "\n",
    "# We make 15 MCDropout iterations to approximate the uncertainty.\n",
    "predictions = baal_model.predict_on_dataset(pool, batch_size=16, iterations=15, use_cuda=USE_CUDA)\n",
    "# We will label the 10 most uncertain samples.\n",
    "top_uncertainty = heuristic(predictions)[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Label those samples.\n",
    "labels = [get_label(train_dataset.files[idx]) for idx in top_uncertainty]\n",
    "active_learning_ds.label(top_uncertainty, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:45] 2019-06-26T20:56:09.768146Z [\u001b[32minfo     ] Starting training              dataset=110 epoch=5\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:54] 2019-06-26T20:56:21.296017Z [\u001b[32minfo     ] Training complete              train_loss=0.06877034902572632\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:80] 2019-06-26T20:56:21.300351Z [\u001b[32minfo     ] Starting evaluating            dataset=1725\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:87] 2019-06-26T20:56:39.271919Z [\u001b[32minfo     ] Evaluation complete            test_loss=0.4217093288898468\n",
      "Metrics: {'train': 0.06877034902572632, 'test': 0.4217093288898468}\n",
      "[14-MainThread   ] [baal.modelwrapper:predict_on_dataset_generator:113] 2019-06-26T20:56:39.275856Z [\u001b[32minfo     ] Start Predict                  dataset=5064\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:45] 2019-06-26T21:00:05.607267Z [\u001b[32minfo     ] Starting training              dataset=120 epoch=5\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:54] 2019-06-26T21:00:18.294080Z [\u001b[32minfo     ] Training complete              train_loss=0.07527568936347961\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:80] 2019-06-26T21:00:18.296946Z [\u001b[32minfo     ] Starting evaluating            dataset=1725\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:87] 2019-06-26T21:00:36.089579Z [\u001b[32minfo     ] Evaluation complete            test_loss=0.33610430359840393\n",
      "Metrics: {'train': 0.07527568936347961, 'test': 0.33610430359840393}\n",
      "[14-MainThread   ] [baal.modelwrapper:predict_on_dataset_generator:113] 2019-06-26T21:00:36.095513Z [\u001b[32minfo     ] Start Predict                  dataset=5054\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:45] 2019-06-26T21:04:02.130017Z [\u001b[32minfo     ] Starting training              dataset=130 epoch=5\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:54] 2019-06-26T21:04:15.289134Z [\u001b[32minfo     ] Training complete              train_loss=0.0781278982758522\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:80] 2019-06-26T21:04:15.293522Z [\u001b[32minfo     ] Starting evaluating            dataset=1725\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:87] 2019-06-26T21:04:32.985569Z [\u001b[32minfo     ] Evaluation complete            test_loss=0.5223127007484436\n",
      "Metrics: {'train': 0.0781278982758522, 'test': 0.5223127007484436}\n",
      "[14-MainThread   ] [baal.modelwrapper:predict_on_dataset_generator:113] 2019-06-26T21:04:32.994347Z [\u001b[32minfo     ] Start Predict                  dataset=5044\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:45] 2019-06-26T21:07:58.095274Z [\u001b[32minfo     ] Starting training              dataset=140 epoch=5\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:54] 2019-06-26T21:08:11.683028Z [\u001b[32minfo     ] Training complete              train_loss=0.10273685306310654\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:80] 2019-06-26T21:08:11.688206Z [\u001b[32minfo     ] Starting evaluating            dataset=1725\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:87] 2019-06-26T21:08:29.385341Z [\u001b[32minfo     ] Evaluation complete            test_loss=0.5248393416404724\n",
      "Metrics: {'train': 0.10273685306310654, 'test': 0.5248393416404724}\n",
      "[14-MainThread   ] [baal.modelwrapper:predict_on_dataset_generator:113] 2019-06-26T21:08:29.389694Z [\u001b[32minfo     ] Start Predict                  dataset=5034\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:45] 2019-06-26T21:11:54.308235Z [\u001b[32minfo     ] Starting training              dataset=150 epoch=5\n",
      "[14-MainThread   ] [baal.modelwrapper:train_on_dataset:54] 2019-06-26T21:12:08.675204Z [\u001b[32minfo     ] Training complete              train_loss=0.049588169902563095\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:80] 2019-06-26T21:12:08.678773Z [\u001b[32minfo     ] Starting evaluating            dataset=1725\n",
      "[14-MainThread   ] [baal.modelwrapper:test_on_dataset:87] 2019-06-26T21:12:26.475939Z [\u001b[32minfo     ] Evaluation complete            test_loss=0.4172689914703369\n",
      "Metrics: {'train': 0.049588169902563095, 'test': 0.4172689914703369}\n",
      "[14-MainThread   ] [baal.modelwrapper:predict_on_dataset_generator:113] 2019-06-26T21:12:26.496595Z [\u001b[32minfo     ] Start Predict                  dataset=5024\n"
     ]
    }
   ],
   "source": [
    "# 5. If not done, go back to 2.\n",
    "for step in range(5): # 5 Active Learning step!\n",
    "  # 2. Train the model for a few epoch on the training set.\n",
    "  baal_model.train_on_dataset(active_learning_ds, optimizer, batch_size=16, epoch=5, use_cuda=USE_CUDA)\n",
    "  baal_model.test_on_dataset(test_dataset, batch_size=16, use_cuda=USE_CUDA)\n",
    "\n",
    "  print(\"Metrics:\", {k:v.avg for k,v in baal_model.metrics.items()})\n",
    "  \n",
    "  # 3. Select the K-top uncertain samples according to the heuristic.\n",
    "  pool = active_learning_ds.pool\n",
    "  if len(pool) == 0:\n",
    "    print(\"We're done!\")\n",
    "    break\n",
    "  predictions = baal_model.predict_on_dataset(pool, batch_size=16, iterations=15, use_cuda=USE_CUDA)\n",
    "  top_uncertainty = heuristic(predictions)[:10]\n",
    "  # 4. Label those samples.\n",
    "  labels = [get_label(train_dataset.files[idx]) for idx in top_uncertainty]\n",
    "  active_learning_ds.label(top_uncertainty, labels)\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And we're done!\n",
    "Be sure to save the dataset and the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "torch.save({\n",
    "  'active_dataset': active_learning_ds.state_dict(),\n",
    "  'model': baal_model.state_dict(),\n",
    "  'metrics': {k:v.avg for k,v in baal_model.metrics.items()}\n",
    "}, '/tmp/baal_output.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Support\n",
    "Submit an issue or reach us to our Gitter!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}